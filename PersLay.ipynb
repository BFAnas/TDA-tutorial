{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersLay",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JKrCJnOsmlGA"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kpge9c6bQJos"
      },
      "source": [
        "Python and compiler versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xmjck8wJQJou",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MKdmKjqQQJoy"
      },
      "source": [
        "Import PersLay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LANIDi770XkZ",
        "colab": {}
      },
      "source": [
        "from PersLay import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7A-jZXHSQJo2"
      },
      "source": [
        "# Reading data\n",
        "\n",
        "In this section, persistence diagrams, features and labels are read from files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmB-tb1-CM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = \"MUTAG\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yub9Upiv-CM3",
        "colab_type": "text"
      },
      "source": [
        "### Persistence diagrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3aqsK3m-CM4",
        "colab_type": "text"
      },
      "source": [
        "Persistence diagrams are assumed to be in **hdf5** file format with filtrations given as keys (i.e. \"alpha\", \"rips\"...) and homological dimensions given as sub-keys (i.e. \"0\", \"1\"...). These keys and sub-keys lead to dictionaries, whose keys correspond to data indexes starting at 0. This **hdf5** file is then read with the **diag_to_dict** function defined in *PersLay.py*. For instance, the 10th 0-dimensional persistence diagram computed with the Rips filtration is accessed with **diagrams[\"rips\"][\"0\"][\"9\"]**, where **diagrams** is the output of **diag_to_dict**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "41Ai5vJNQJpB",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "\n",
        "filts = [\"Ord0_10.0-hks\", \"Rel1_10.0-hks\", \"Ext0_10.0-hks\", \"Ext1_10.0-hks\"]\n",
        "path_to_diag = dataset + \"/\" + dataset + \".hdf5\"\n",
        "diag = diag_to_dict(h5py.File(path_to_diag, \"r\"), filts=filts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4WowFoQQJpF"
      },
      "source": [
        "Print name and cardinality of the different filtrations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "htasllN5zHNY",
        "colab": {}
      },
      "source": [
        "for filt in diag.keys():\n",
        "    print(\"Max cardinal of filtration \" + filt + \" = \" + str(max([len(dgm) for dgm in diag[filt]])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JnbLhSuXQJpR"
      },
      "source": [
        "Visualize the persistence diagrams (optional, requires **sklearn_tda**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CrGR3rabQJpS",
        "colab": {}
      },
      "source": [
        "import sklearn_tda as tda\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "filtration   = \"Ord0_10.0-hks\"    # Name of filtration\n",
        "idx_diagram  = 0          # Index of persistence diagram that will be visualized\n",
        "\n",
        "# Retrieve points with finite coordinates\n",
        "diag_example = tda.DiagramSelector(limit=np.inf).fit_transform(diag[filtration])\n",
        "\n",
        "# Get minimum and maximum coordinates for all diagrams in the filtration\n",
        "pre = tda.DiagramPreprocessor(use=True, scalers=[([0,1], MinMaxScaler())]).fit(diag_example)\n",
        "[mx,my],[Mx,My] = pre.scalers_[0][1].data_min_, pre.scalers_[0][1].data_max_\n",
        "print(\"Minimum x = \" + str(mx) + \", Maximum x = \" + str(Mx) + \\\n",
        "      \", Minimum y = \" + str(my) + \", Maximum y = \" + str(My))\n",
        "\n",
        "# Plot persistence diagram corresponding to given index\n",
        "xs, ys = diag_example[idx_diagram][:,0], diag_example[idx_diagram][:,1]\n",
        "plt.scatter(xs,ys)\n",
        "plt.plot([min(xs),max(xs)],[min(xs),max(xs)])\n",
        "plt.axis([min(xs),max(xs),min(ys),max(ys)])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYCXVpvU-CND",
        "colab_type": "text"
      },
      "source": [
        "### Features and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbQ_m8Ig-CND",
        "colab_type": "text"
      },
      "source": [
        "Features are given in **csv** file format. The first column is assumed to be called *label* and to contain the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V1UU2I1TQJo_",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "path_to_feat = \"gdrive/My Drive/data/\" + dataset + \"/\" + dataset + \".csv\"\n",
        "feat = pd.read_csv(path_to_feat, index_col=0, header=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UIzdJZUEQJpK"
      },
      "source": [
        "Read and encode labels and features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "THDmrndkQJpL",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Extract and encode labels with integers\n",
        "L = np.array(feat[\"label\"])\n",
        "L = np.array(LabelEncoder().fit_transform(L))\n",
        "L = OneHotEncoder(sparse=False, categories=\"auto\").fit_transform(L[:,np.newaxis]) \n",
        "num_labels = L.shape[1]\n",
        "\n",
        "# Extract features\n",
        "F = np.array(feat)[:,1:]\n",
        "[num_pts, num_features] = F.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IDsop-aDQJpO"
      },
      "source": [
        "Print summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WWGD8sEQJpP",
        "colab": {}
      },
      "source": [
        "print(str(num_pts) + \" instances and \" + str(num_features) + \" features.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G1oB9SMOQJpV"
      },
      "source": [
        "# (Optional) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_vJLrRlmQJpX"
      },
      "source": [
        "In this section, persistence diagrams are preprocessed with **sklearn_tda** (see https://github.com/MathieuCarriere/sklearn_tda). If you already preprocessed your diagrams, you can skip this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KInh-_j2-CNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn_tda as tda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1fFDpa9-CNT",
        "colab_type": "text"
      },
      "source": [
        "Use sklearn format pipelines, can fit and fit_transform and apply sequences of args. For example, Separator selects finite or essential points, ProminentPts (or Quantizer) forces uniformly upper bounded number of points and MinMax rescales diagrams to have coordinates in [0,1] x [0,1]. At the end, all diagrams are represented by a (Npts x Ndgm x dim+1) array, where Npts is the number of instances, Ndgm is the upper bound on the number of points, dim is the dimension of the diagrams (usually 2 for finite persistence diagrams and 1 for essential ones). The last dimension is dim+1 because we give an additional coordinate for the points in the diagrams specifying if the point is relevant (in which case the value of this coordinate is 1) or added after padding (in which case the value of the coordinate is 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC0Hu9xK-CNU",
        "colab_type": "text"
      },
      "source": [
        "*Remark:* **nu_separator** operation is used only for baseline experiment on graphs in the article \"Deep Learning with Topological Signatures\". Use it only if you wish to work with their architecture, that is implemented in the next section as a special case of PersLay channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzM_Kl3x-CNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Definition of our scaler for the persistence diagrams:\n",
        "# 1. transform the diagram points with (x,y) ---> (x, y-x) \n",
        "# 2. rescale the diagrams to the unit square [0,1] x [0,1]\n",
        "scaler = [([0,1],  Pipeline([(\"1\", tda.BirthPersistenceTransform()), (\"2\", MinMaxScaler()) ]))]\n",
        "\n",
        "# Whole pipeline\n",
        "preprocess = Pipeline([\n",
        "    (\"Selector\",      tda.DiagramSelector(use=True, point_type=\"finite\")),\n",
        "    (\"ProminentPts\",  tda.ProminentPoints(use=True, num_pts=400, point_type=\"finite\")),\n",
        "    (\"Scaler\",        tda.DiagramPreprocessor(use=True,  scalers=scaler)),\n",
        "    (\"NuSeparator\",   tda.DiagramPreprocessor(use=False, scalers=[([0,1],nu_separator(nu=.1))])),\n",
        "    (\"Padding\",       tda.Padding(use=True)),\n",
        "                      ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y8qqWJLdQJpa"
      },
      "source": [
        "Apply the previous pipeline on the different filtrations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "heQZrZjZQJpb",
        "colab": {}
      },
      "source": [
        "# Number of points to keep for each filtration\n",
        "prm = {\"Ord0_10.0-hks\":  {\"ProminentPts__num_pts\": 400}, \"Rel1_10.0-hks\":  {\"ProminentPts__num_pts\": 400},\n",
        "       \"Ext0_10.0-hks\":  {\"ProminentPts__num_pts\": 400}, \"Ext1_10.0-hks\":  {\"ProminentPts__num_pts\": 400}}\n",
        "\n",
        "# Apply pipeline\n",
        "D = []\n",
        "for dt in prm.keys():\n",
        "    param = prm[dt]\n",
        "    preprocess.set_params(**param)\n",
        "    D.append(preprocess.fit_transform(diag[dt]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1XrwYwLQolBi"
      },
      "source": [
        "For each filtration, concatenate all diagrams in a single array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwuPSduBnaZq",
        "colab": {}
      },
      "source": [
        "D_pad = []\n",
        "for dt in range(len(prm.keys())):\n",
        "    D_pad.append(np.concatenate([D[dt][i][np.newaxis,:] for i in range(len(D[dt]))], axis=0))\n",
        "    print(D_pad[dt].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J6q_dnrgzN2N"
      },
      "source": [
        "Visualize the preprocessed persistence diagrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2a6Ibzv-CNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "filtration   = 1\n",
        "idx_diagram  = 2\n",
        "\n",
        "diag_example = D[filtration]\n",
        "pre = tda.DiagramPreprocessor(use=True, scalers=[([0,1,2], MinMaxScaler())]).fit(diag_example)\n",
        "[mx,my,mz],[Mx,My,Mz] = pre.scalers_[0][1].data_min_, pre.scalers_[0][1].data_max_\n",
        "print(\"Minimum x = \" + str(mx) + \", Maximum x = \" + str(Mx) + \", Minimum y = \" + str(my) + \", Maximum y = \" + str(My))\n",
        "\n",
        "xs, ys = diag_example[idx_diagram][:,0], diag_example[idx_diagram][:,1]\n",
        "plt.scatter(xs,ys)\n",
        "plt.axis([min(xs),max(xs),min(ys),max(ys)])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jHG136kvQJpj"
      },
      "source": [
        "# Classification\n",
        "\n",
        "In this section, we classify persistence diagrams with PersLay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPl48u7b-CNs",
        "colab_type": "text"
      },
      "source": [
        " **Warning:** persistence diagrams are assumed to be preprocessed, i.e., they should all have the same number of points, and the points in the persistence diagrams should have a third coordinate, which is either 1 or 0, and which indicates if the point is meaningful (1) or if it has been added after padding (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t0jjFnU_QJpt"
      },
      "source": [
        "### Architecture of neural network\n",
        "In this subsection, we define the neural network architecture with several **PersLay** channels. We also provide a specific PersLay architecture that implements the network defined in https://arxiv.org/pdf/1707.04041.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r7IOOQz7pXE1"
      },
      "source": [
        "Neural network with several **PersLay** channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wS4tLcUIQJpy",
        "colab": {}
      },
      "source": [
        "from tensorflow import random_uniform_initializer as rui\n",
        "\n",
        "def model(indxs, feats, diags):\n",
        "\n",
        "    list_v = []\n",
        "    \n",
        "    # Channels for persistence diagrams. There is one per filtration.\n",
        "    PersLay(list_v, \"fin-0\", diags[0],\n",
        "        #layer=\"ls\", num_samples=25, perm_op=\"topk\", keep=2,\n",
        "        #layer=\"gs\", num_gaussians=25, perm_op=\"sum\", mean_init=rui(0.,1.), variance_init=rui(1.,1.), mean_const=False, variance_const=False,\n",
        "        layer=\"pm\", peq=[(2,None)], perm_op=\"sum\", keep=5, weight_init=rui(0.,1.), bias_init=rui(-1.,1.),\n",
        "        #layer=\"im\", image_size=[5,5], perm_op=\"sum\", image_bnds=[[0.,1.],[0.,1.]], variance_init=rui(1.,1.),\n",
        "        persistence_weight=\"linear\", grid_size=[5,5], grid_bnds=[[0.,1.],[0.,1.]], grid_init=rui(1.,1.),\n",
        "        fc_layers=[],\n",
        "        cv_layers=[(10,2,\"bdr\")],\n",
        "        tensor=True)\n",
        "    PersLay(list_v, \"fin-1\", diags[1],\n",
        "        #layer=\"ls\", num_samples=25, perm_op=\"topk\", keep=2,\n",
        "        #layer=\"gs\", num_gaussians=25, perm_op=\"sum\", mean_init=rui(0.,1.), variance_init=rui(1.,1.), mean_const=False, variance_const=False,\n",
        "        layer=\"pm\", peq=[(2,None)], perm_op=\"sum\", keep=5, weight_init=rui(0.,1.), bias_init=rui(-1.,1.),\n",
        "        #layer=\"im\", image_size=[5,5], perm_op=\"sum\", image_bnds=[[0.,1.],[0.,1.]], variance_init=rui(1.,1.),\n",
        "        persistence_weight=\"linear\", grid_size=[5,5], grid_bnds=[[0.,1.],[0.,1.]], grid_init=rui(1.,1.),\n",
        "        fc_layers=[],\n",
        "        cv_layers=[(10,2,\"bdr\")],\n",
        "        tensor=True)\n",
        "    PersLay(list_v, \"fin-2\", diags[2],\n",
        "        #layer=\"ls\", num_samples=25, perm_op=\"topk\", keep=2,\n",
        "        #layer=\"gs\", num_gaussians=25, perm_op=\"sum\", mean_init=rui(0.,1.), variance_init=rui(1.,1.), mean_const=False, variance_const=False,\n",
        "        layer=\"pm\", peq=[(2,None)], perm_op=\"sum\", keep=5, weight_init=rui(0.,1.), bias_init=rui(-1.,1.),\n",
        "        #layer=\"im\", image_size=[5,5], perm_op=\"sum\", image_bnds=[[0.,1.],[0.,1.]], variance_init=rui(1.,1.),\n",
        "        persistence_weight=\"linear\", grid_size=[5,5], grid_bnds=[[0.,1.],[0.,1.]], grid_init=rui(1.,1.),\n",
        "        fc_layers=[],\n",
        "        cv_layers=[(10,2,\"bdr\")],\n",
        "        tensor=True)\n",
        "    PersLay(list_v, \"fin-3\", diags[3],\n",
        "        #layer=\"ls\", num_samples=25, perm_op=\"topk\", keep=2,\n",
        "        #layer=\"gs\", num_gaussians=25, perm_op=\"sum\", mean_init=rui(0.,1.), variance_init=rui(1.,1.), mean_const=False, variance_const=False,\n",
        "        layer=\"pm\", peq=[(2,None)], perm_op=\"sum\", keep=5, weight_init=rui(0.,1.), bias_init=rui(-1.,1.),\n",
        "        #layer=\"im\", image_size=[5,5], perm_op=\"sum\", image_bnds=[[0.,1.],[0.,1.]], variance_init=rui(1.,1.),\n",
        "        persistence_weight=\"linear\", grid_size=[5,5], grid_bnds=[[0.,1.],[0.,1.]], grid_init=rui(1.,1.),\n",
        "        fc_layers=[],\n",
        "        cv_layers=[(10,2,\"bdr\")],\n",
        "        tensor=True)\n",
        "        \n",
        "    # Concatenate all channels and add other features\n",
        "    vector = tf.concat(list_v, 1)\n",
        "    with tf.variable_scope(\"norm_diag\"):\n",
        "        vector = tf.layers.batch_normalization(vector)\n",
        "    with tf.variable_scope(\"norm_feat\"):\n",
        "        feat = tf.layers.batch_normalization(feats)\n",
        "    vector = tf.concat([vector, feat[:,:]], 1)\n",
        "    \n",
        "    # Final fully-connected layer\n",
        "    with tf.variable_scope(\"final-dense-3\"):\n",
        "        vector = post_processing(tf.layers.dense(vector, num_labels), \"\")\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RjJaPfM3QJpu"
      },
      "source": [
        "**Example of neural network from \"Deep Learning with Topological Signatures\".** Don't run this if you want to run your own model (defined in cell above). This cell implements the architecture defined in https://arxiv.org/pdf/1707.04041.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ksGgZuoSQJpu",
        "colab": {}
      },
      "source": [
        "def baseline_model(feats, diags):\n",
        "    list_v = []\n",
        "    PersLay(list_v, \"fin0\", diags[0],\n",
        "        layer=\"gs\", num_gaussians=150, perm_op=\"sum\",\n",
        "        persistence_weight=None,\n",
        "        fc_layers=[(75,\"bd\"),(75,\"rd\")])\n",
        "    PersLay(list_v, \"ess0\", diags[1],\n",
        "        layer=\"gs\", num_gaussians=50, perm_op=\"sum\",\n",
        "        persistence_weight=None,\n",
        "        fc_layers = [(25,\"bd\"),(25,\"rd\")])\n",
        "    PersLay(list_v, \"ess1\", diags[2],\n",
        "        layer=\"gs\", num_gaussians=50, perm_op=\"sum\",\n",
        "        persistence_weight=None,\n",
        "        fc_layers = [(25,\"bd\"),(25,\"rd\")])\n",
        "    vector = tf.concat(list_v, 1)\n",
        "    with tf.variable_scope(\"final-dense-0\"):\n",
        "        vector = post_processing(tf.layers.dense(vector, 200), \"br\")\n",
        "    with tf.variable_scope(\"final-dense-1\"):\n",
        "        vector = post_processing(tf.layers.dense(vector, 100), \"bdr\")\n",
        "    with tf.variable_scope(\"final-dense-2\"):\n",
        "        vector = post_processing(tf.layers.dense(vector, 50), \"br\")\n",
        "    with tf.variable_scope(\"final-dense-3\"):\n",
        "        vector = post_processing(tf.layers.dense(vector, num_labels), \"b\")\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GiHb0QLrQJp0"
      },
      "source": [
        "### Train and test data\n",
        "In this subsection, we finally train and test the network on the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qy5I2EL-CN8",
        "colab_type": "text"
      },
      "source": [
        "Specify here how you want to train data and test data: either with K-folds (\"KF\") or with random permutations of test set (\"RP\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UMXDGkReQJp2",
        "colab": {}
      },
      "source": [
        "mode        = \"KF\"     # Either \"KF\" or \"RP\"\n",
        "num_scores  = 10        # Number of score generations\n",
        "num_splits  = 10      # Number of splits\n",
        "test_size   = 0.3      # Size of test set in case of \"RP\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0gjtY_p-COA",
        "colab_type": "text"
      },
      "source": [
        "Specify here if you have one or several GPUs or CPUs, as well as number of epochs, batch size and validation size. If you do not want to use validation sets for early stopping, set **valid_size** to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y4939fL-COA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tower   = 1        # Number of computing units\n",
        "tower_type  = \"gpu\"    # Type of computing units (\"cpu\" or \"gpu\")\n",
        "batch_size  = 128      # Batch size for each tower\n",
        "num_epochs  = 1000      # Number of epochs\n",
        "valid_size  = 0.       # Size of validation set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI6rOz0x-COD",
        "colab_type": "text"
      },
      "source": [
        "Specify here the decay of Exponential Moving Average, the learning rate of optimizer and the verbose for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93DPGFBt-COE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decay       = 0.9       # Decay of Exponential Moving Average\n",
        "learn_rate  = 0.01     # Learning rate of optimizer\n",
        "verbose     = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9qV87By-COH",
        "colab_type": "text"
      },
      "source": [
        "Train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XqeSy10-COI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, ShuffleSplit\n",
        "\n",
        "for idx_score in range(num_scores):\n",
        "\n",
        "    if mode == \"KF\": # Evaluation with k-fold on test set\n",
        "        folds = KFold(n_splits=num_splits, random_state=idx_score, shuffle=True).split(np.empty([num_pts]))\n",
        "    if mode == \"RP\": # Evaluation with random test set\n",
        "        folds = ShuffleSplit(n_splits=num_splits, test_size=test_size, random_state=idx_score).split(np.empty([num_pts]))\n",
        "\n",
        "    train_accs, valid_accs, test_accs = np.zeros([num_splits, num_epochs]), np.zeros([num_splits, num_epochs]), np.zeros([num_splits, num_epochs])\n",
        "    for idx, (train_sub, test_sub) in enumerate(folds):\n",
        "      \n",
        "        valid_sub = train_sub[:int(valid_size*len(train_sub))]\n",
        "        train_sub = train_sub[int(valid_size*len(train_sub)):]\n",
        "\n",
        "        print(str(len(train_sub)) + \" train points and \" + str(len(test_sub)) + \" test points\")\n",
        "\n",
        "        # Create neural network\n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        # Evaluation of neural network\n",
        "        ltrain, lvalid, ltest = evaluate_nn_model(L,F,D_pad,train_sub,valid_sub,test_sub,\\\n",
        "                                                  model,num_tower,tower_type,num_epochs,\\\n",
        "                                                  decay,learn_rate,batch_size,verbose)\n",
        "        \n",
        "        train_accs[idx,:],valid_accs[idx,:],test_accs[idx,:] = np.array(ltrain),np.array(lvalid),np.array(ltest)\n",
        "    \n",
        "        # Write results\n",
        "        #np.save(dataset + \"_train_score\" + str(idx_score) + \"_split\"  + str(idx), np.array(ltrain))\n",
        "        #np.save(dataset + \"_valid_score\" + str(idx_score) + \"_split\"  + str(idx), np.array(lvalid))\n",
        "        np.save(dataset + \"/\" + dataset + \"_test_score\"  + str(idx_score) + \"_split\"  + str(90+idx), np.array(ltest))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}